{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bfa948",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc830f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.first get the webpage https://www.naukri.com/\n",
    "\n",
    "2.Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "3.Then click the search button.\n",
    "\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5.Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bde6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = driver.find_element(by.CLASS_NAME\"Suggestor_input\")\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = driver.find_element(by.XPATH,'')\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ef0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Analyst')\n",
    "driver.find_element_by_xpath('//*[@id=\"qsb-location-sugg\"]').send_keys('Bangalore')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements_by_xpath('//div[@class=\"info fleft\"]/a')[:10]\n",
    "company_title=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dab25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7021b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3fa553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/033[1mTop 10 Data Analyst Job at Bangrole :/033[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>JOB LOCATION</th>\n",
       "      <th>Experience Require</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [JOB TITLE, COMPANY, JOB LOCATION, Experience Require]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Data_analyst=pd.DataFrame({})\n",
    "Data_analyst['JOB TITLE']='job_title'\n",
    "Data_analyst['COMPANY']='company'\n",
    "Data_analyst['JOB LOCATION']='job_location'\n",
    "Data_analyst['Experience Require']='experienced'\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_analyst[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 Data Analyst   Marketing  byjus    Transforming Education using Technology Bangalore/Bengaluru   0-2 Yrs\n",
    "1 Business Data Analyst  INDIAVIDUAL LEARNING LIMITED   Bangalore/Bengaluru  2-6 Yrs\n",
    "2 Lead Data Analyst / Data Engineer  Huawei Technologies  Bangalore/Bengaluru   5-10 Yrs\n",
    "3 Data Analyst / Business Analyst (Demand Planning) Flipkart  Bangalore/Bengaluru  1-4 Yrs\n",
    "4 Senior Data Analyst  Scienaptic Systems Bangalore/Bengaluru  5-10 Yrs\n",
    "5 Data Analyst  Applied Materials  Bangalore/Bengaluru  0-3 Yrs\n",
    "6 Business Data Analys  tRANDSTAD INDIA PVT LTD   Bangalore/Bengaluru  8-12 Yrs\n",
    "7 Data Analyst - I/II  Philips India Limited  Bangalore/Bengaluru   3-6 Yrs\n",
    "9 Openings For Data Analyst  Allegis Services India Pvt. Ltd.  Bangalore/Bengaluru  0-3 Yrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5ac72",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "This task will be done in following steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "\n",
    "2.Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "3.Then click the search button.\n",
    "\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5.Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.naukri.com/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Scientist')\n",
    "driver.find_element_by_xpath('//*[@id=\"qsb-location-sugg\"]').send_keys('Bangalore')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "job_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "for i in job_tags:\n",
    "    URL.append(i.get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e83ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        job_title.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/header/h1')).text)\n",
    "        job_location.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[2]/div[3]/span')).text)\n",
    "        company_name.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/div/a')).text)\n",
    "        job_description.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[2]')).text.replace(\"\\n\",\"\"))\n",
    "    except NoSuchElementException:\n",
    "        job_title.append(\"Nan\")\n",
    "        job_location.append(\"Nan\")\n",
    "        company_name.append(\"Nan\")\n",
    "        job_description.append(\"Nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12746b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['job_title']='job_title'\n",
    "Data_scientist['job_location']='job_location'\n",
    "Data_scientist['company_name']='company_name'\n",
    "Data_scientist['job_description']='job_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e63105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTop 10 Data Analyst Job at Bangrole :\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'\\033[0m')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72018d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "      job_title                   job_location                          company_name                               job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "0  Lead Data Scientist         Bangalore/Bengaluru           Huawei Technologies                             job descriptionWe are looking for a Lead data ...\n",
    "1  Sr. Data Scientist / Data Scientist For Bangal...         Kolkata, Bangalore/BengalurumPokket             Job descriptionJob Description:1. Must havea. ...\n",
    "2  Senior Data Scientist   Noida, Bangalore/Bengaluru         InnovAccer                                     Job description A day in the life at Innovacc...\n",
    "3  Senior Data- Python/Machin.    Mumbai, Hyderabad/Secunderabad, Pune, Bangal.Altimax Business Solution     job descriptionAbout The Job :- You will archi...\n",
    "4  Data Scientist/Senior Data Scientist - IT Serv... Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...Pylon Management Consulting Pvt Ltd\tJob descriptionResponsibilities :- Analytics l...\n",
    "5  SDE2 Data Scientist  Bangalore/Bengaluru Multi Recruit Job descriptionUse data to develop machine lea...\n",
    "6  SDE1 Data Scientist  Bangalore/Bengaluru Multi Recruit Job descriptionUse data to develop machine lea...\n",
    "7  Data Scientist       Bangalore/Bengaluru epiFi Technologies Job descriptionAt epiFi you will :Research and...\n",
    "8  Data Scientist       Pune, Bangalore/Bengaluru  Nutanix India Technologies Private Limited  Job descriptionObjectives of this RoleCollabor...\n",
    "9  Nan                       Nan                                       Nan                                  Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698eceb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5e6359c",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on thewebpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” . The salary filter to be used is “3-6” lakhs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a17c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "\n",
    "2.Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "\n",
    "3.Then click the search button.\n",
    "\n",
    "4.Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "5.Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "6.Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.naukri.com/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ae470",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Scientist')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/p/span[1]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88920d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/p/span[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c295cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b293b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')[:10]\n",
    "company_title=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d34d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d161a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/033[1mTop 10 Data Analyst Job at Bangrole :/033[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>JOB LOCATION</th>\n",
       "      <th>Experience Require</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [JOB TITLE, COMPANY, JOB LOCATION, Experience Require]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['JOB TITLE']='job_title'\n",
    "Data_scientist['COMPANY']='company'\n",
    "Data_scientist['JOB LOCATION']='job_location'\n",
    "Data_scientist['Experience Require']='experienced'\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_scientist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbea6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "0  Job Opportunity || Data Scientist || HCL Techn... HCL Technologies    Delhi / NCR     4-7 Yrs\n",
    "1  Associate Data Scientist   Optum Global Solutions (India) Private Limited  Noida      2-7 Yrs\n",
    "2  Data Scientist I/II/III    OLX India Pvt Ltd Gurgaon/Gurugram                           3-6 Yrs\n",
    "3  Academic Counsellor   Data Scientist GreatLearning  Gurgaon/Gurugram, Bangalore/Bengaluru 1-4 Yrs\n",
    "4  Data Scientist       lericon infomatics pvt.ltd     Mumbai, Delhi / NCR                    3-5 Yrs\n",
    "5  Data Scientist       Optum Global Solutions (India) Private Limited Noida, Gurgaon/Gurugram  2-6 Yrs\n",
    "6  Data Scientist       Milliman India Pvt Ltd      Gurgaon/Gurugram                            2-5 Yrs\n",
    "7  Data Scientist       Fractal Analytics            Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru 3-7 Yrs\n",
    "8  Data Scientist Internship  iHackers Inc            New Delhi                                     0-1 Yrs\n",
    "9  Data Scientist    NatWest Group                     Gurgaon/Gurugram                              2-7 Yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98411f59",
   "metadata": {},
   "source": [
    "# Q4 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "4.Discount %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1.Go to flipkart webpage by url https://www.flipkart.com/\n",
    "\n",
    "2.Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "\n",
    "3.after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "\n",
    "4.after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it.\n",
    "\n",
    "5.Now scrape data from this page as usual\n",
    "\n",
    "6.repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ed3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//div[@class=\"_3OO5Xc\"]/input').send_keys('sunglasses')\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5563427",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "for i in range(0,3):\n",
    "    S_g_url=driver.find_elements_by_xpath('//a[@class=\"_2UzuFa\"]')\n",
    "    for i in S_g_url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    # Clicking on NEXT button at end of page\n",
    "    driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "    time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20811bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24703b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL[:100]:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath(\"//span[@class='G6XhRU']\").text\n",
    "        Brand.append(brand)\n",
    "        description=driver.find_element_by_xpath(\"//span[@class='B_NuCI']\").text\n",
    "        Product_Description.append(description)\n",
    "        price=driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\").text\n",
    "        Price.append(price)\n",
    "        discount=driver.find_element_by_xpath(\"//div[@class='_3Ay6Sb _31Dcoz pZkvcx']/span\").text\n",
    "        Discount.append(discount)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"NaN\")\n",
    "        Product_Description.append(\"NaN\")\n",
    "        Price.append(\"NaN\")\n",
    "        Discount.append('NaN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721be1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand, Product_Description, Price, Discount]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']='Brand'[:100]\n",
    "Sunglasses['Product_Description']='Product_Description'[:100]\n",
    "Sunglasses['Price']='Price'[:100]\n",
    "Sunglasses['Discount']='Discount'[:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38a850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b9159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4f312c",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "\n",
    "You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "\n",
    "\n",
    "When you will open the above link you will reach to the below shown webpage. As shown in the above page you have to scrape the tick marked attributes. These are\n",
    "\n",
    "1.Rating\n",
    "2.Review_summary\n",
    "3.Full review\n",
    "\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c16c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "#creating empty lists\n",
    "Rating=[]\n",
    "Review_summary=[]\n",
    "Full_review=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,12):\n",
    "        \n",
    "    rat = driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    rev = driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "    frev = driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "\n",
    "    for k in rat:\n",
    "        rating.append(k.text)\n",
    "\n",
    "    for i in rev:\n",
    "        review_summary.append(i.text)\n",
    "\n",
    "    for j in frev:\n",
    "        full_review.append(j.text)\n",
    "    time.sleep(3)\n",
    "\n",
    "    print(len(review_summary))\n",
    "    \n",
    "    nxt_page = driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_page[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_page[0].get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9503e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+'Iphone 11 Reviews from Flipkart :'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18f98d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rating, Review Summary, Full Review]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review_Iphone11= pd.DataFrame({})\n",
    "Review_Iphone11['Rating'] = 'rating'[:100]\n",
    "Review_Iphone11['Review Summary'] = 'review_summary'[:100] \n",
    "Review_Iphone11['Full Review'] = 'full_review'[:100]\n",
    "Review_Iphone11.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209760ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111b4784",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "You have to scrape 4 attributes of each sneaker :\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "4.discount %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e2d8c",
   "metadata": {},
   "source": [
    "# As shown in the below image, you have to scrape the tick marked attributes.Also note that all the steps required during scraping should be done through code only and not manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//div[@class=\"_3OO5Xc\"]/input').send_keys('sneakers')\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebdcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e494bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "for i in range(0,4):\n",
    "    sneakers=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in sneakers:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    driver.find_element_by_xpath('//a[@class=\"ge-49M\"]').click()\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_url=URL[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(list_of_url):\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath(\"//span[@class='G6XhRU']\").text\n",
    "        Brand.append(brand)\n",
    "        description=driver.find_element_by_xpath(\"//span[@class='B_NuCI']\").text\n",
    "        Product_Description.append(description)\n",
    "        price=driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\").text\n",
    "        Price.append(price)\n",
    "        discount=driver.find_element_by_xpath(\"//div[@class='_3Ay6Sb _31Dcoz pZkvcx']/span\").text\n",
    "        Discount.append(discount)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"Null\")\n",
    "        Product_Description.append(\"Null\")\n",
    "        Price.append(\"Null\")\n",
    "        Discount.append('Null')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c6f9481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 19 5 8\n"
     ]
    }
   ],
   "source": [
    "print(len('Brand'),len('Product_Description'),len('Price'),len('Discount'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93322b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand, Product_Description, Price, Discount]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sneakers=pd.DataFrame({})\n",
    "Sneakers['Brand']='Brand'[:100]\n",
    "Sneakers['Product_Description']='Product_Description'[:100]\n",
    "Sneakers['Price']='Price'[:100]\n",
    "Sneakers['Discount']='Discount'[:100]\n",
    "Sneakers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3770c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
